{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pool:  mean\n",
      "pool:  mean\n",
      "pool:  mean\n",
      "pool:  mean\n",
      "pool:  mean\n",
      "pool:  mean\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeepSet(\n",
       "  (sequential): ModuleList(\n",
       "    (0): DeepSetLayer(\n",
       "      (Gamma): Linear(in_features=5, out_features=80, bias=True)\n",
       "      (Lambda): Linear(in_features=5, out_features=80, bias=True)\n",
       "      (bn): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "    )\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): DeepSetLayer(\n",
       "      (Gamma): Linear(in_features=80, out_features=120, bias=True)\n",
       "      (Lambda): Linear(in_features=80, out_features=120, bias=True)\n",
       "      (bn): BatchNorm1d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "    )\n",
       "    (3): LeakyReLU(negative_slope=0.01)\n",
       "    (4): DeepSetLayer(\n",
       "      (Gamma): Linear(in_features=120, out_features=70, bias=True)\n",
       "      (Lambda): Linear(in_features=120, out_features=70, bias=True)\n",
       "      (bn): BatchNorm1d(70, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "    )\n",
       "    (5): LeakyReLU(negative_slope=0.01)\n",
       "    (6): DeepSetLayer(\n",
       "      (Gamma): Linear(in_features=70, out_features=50, bias=True)\n",
       "      (Lambda): Linear(in_features=70, out_features=50, bias=True)\n",
       "      (bn): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "    )\n",
       "    (7): LeakyReLU(negative_slope=0.01)\n",
       "    (8): DeepSetLayer(\n",
       "      (Gamma): Linear(in_features=50, out_features=8, bias=True)\n",
       "      (Lambda): Linear(in_features=50, out_features=8, bias=True)\n",
       "      (bn): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "    )\n",
       "    (9): DeepSetLayer(\n",
       "      (Gamma): Linear(in_features=8, out_features=2, bias=True)\n",
       "      (Lambda): Linear(in_features=8, out_features=2, bias=True)\n",
       "      (bn): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from deepset import *\n",
    "MODEL = DeepSet(in_features=5, feats=[80,120,70,50,8], n_class=2,pool=\"mean\") #for full dataset Mean pooling, for small dataset Max pooling\n",
    "model=MODEL\n",
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model=model.to(device) \n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "class StreamingHcaDataset(Dataset): \n",
    "    def __init__(self, proton_dir, pion_dir, features=[\"x\", \"y\", \"z\", \"total_energy\", \"mean_time\"]):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.proton_files = sorted([os.path.join(proton_dir, f) for f in os.listdir(proton_dir) if f.endswith(\".parquet\")])\n",
    "        self.pion_files = sorted([os.path.join(pion_dir, f) for f in os.listdir(pion_dir) if f.endswith(\".parquet\")])\n",
    "\n",
    "        self.features = features\n",
    "        self.all_files = self.proton_files + self.pion_files  # Combine file lists\n",
    "        self.labels = [0] * len(self.proton_files) + [1] * len(self.pion_files)  # 0 for proton, 1 for pion\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_files)  # Total number of files\n",
    "\n",
    "    def _load_file(self, file_path, label):\n",
    "        \"\"\"Loads a single pickle file (containing a single DataFrame) and returns point cloud data with labels.\"\"\"\n",
    "        # with open(file_path, \"rb\") as f:\n",
    "        #     df = pickle.load(f)  # Load single DataFrame\n",
    "    \n",
    "        # df=pd.read_pickle(file_path)\n",
    "        df=pd.read_parquet(file_path)\n",
    "        df=df[df[\"total_energy\"]>5]\n",
    "        \n",
    "          \n",
    "        part_feat = df[self.features].to_numpy()\n",
    "\n",
    "        # Handle NaN and Inf values\n",
    "        part_feat[np.isnan(part_feat)] = 0.0\n",
    "        part_feat[np.isinf(part_feat)] = 0.0\n",
    "\n",
    "        return {\n",
    "            \"part\": torch.tensor(part_feat, dtype=torch.float32),\n",
    "            \"label\": torch.tensor(label, dtype=torch.long),\n",
    "            \"seq_length\": torch.tensor(part_feat.shape[0], dtype=torch.long),\n",
    "        }\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        random_idx = random.randint(0, len(self.all_files) - 1)  # Pick a random file\n",
    "        file_path = self.all_files[random_idx]\n",
    "        label = self.labels[random_idx]\n",
    "\n",
    "        return self._load_file(file_path, label)  # Return data from the chosen file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Custom collate function to handle variable-length point cloud data.\"\"\"\n",
    "    parts = [item[\"part\"] for item in batch]  # List of tensors (each of shape [N, 5])\n",
    "    labels = torch.tensor([item[\"label\"] for item in batch], dtype=torch.long)  # Convert list to tensor\n",
    "    seq_lengths = torch.tensor([item[\"seq_length\"] for item in batch], dtype=torch.long)  # Convert list to tensor\n",
    "\n",
    "    # Pad variable-length tensors to the longest sequence in the batch\n",
    "    padded_parts = pad_sequence(parts, batch_first=True, padding_value=0.0)  # Shape [batch_size, max_seq_len, 5]\n",
    "\n",
    "    return {\"part\": padded_parts, \"label\": labels, \"seq_length\": seq_lengths}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 160000, Validation: 20000, Test: 20000\n"
     ]
    }
   ],
   "source": [
    "# positron_dir = \"/mnt/newdisk/2003_data_pkl/PKL_poistron/PKL_positron\"\n",
    "# proton_dir = \"/mnt/newdisk/2003_data_pkl/PKL_proton/PKL_proton\"\n",
    "positron_dir = \"/mnt/c/Users/hnayak/Documents/small_PKL_pion_100GeV_50\"\n",
    "proton_dir = \"/mnt/c/Users/hnayak/Documents/small_PKL_proton_100GeV_50\"\n",
    "\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "\n",
    "# Define dataset\n",
    "train_dataset = StreamingHcaDataset(proton_dir=proton_dir, pion_dir=positron_dir)\n",
    "\n",
    "# Define split sizes\n",
    "total_size = len(train_dataset)\n",
    "train_size = int(0.8 * total_size)\n",
    "val_size = int(0.1 * total_size)\n",
    "test_size = total_size - train_size - val_size  # Ensure the sum matches total_size\n",
    "\n",
    "# Split dataset\n",
    "train_set, val_set, test_set = random_split(train_dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# Define DataLoaders\n",
    "train_loader = DataLoader(train_set, batch_size=32, shuffle=True, collate_fn=collate_fn, num_workers=28)\n",
    "val_loader = DataLoader(val_set, batch_size=32, shuffle=False, collate_fn=collate_fn,num_workers=28)\n",
    "test_loader = DataLoader(test_set, batch_size=32, shuffle=False, collate_fn=collate_fn,num_workers=28)\n",
    "\n",
    "\n",
    "print(f\"Train: {train_size}, Validation: {val_size}, Test: {test_size}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "{'part': tensor([[[50.0000, 49.0000,  0.0000, 18.5746,  1.3747],\n",
      "         [49.0000, 49.0000,  0.0000,  6.2649,  1.4144],\n",
      "         [49.0000, 49.0000,  1.0000, 24.8078,  1.4616],\n",
      "         ...,\n",
      "         [50.0000, 50.0000,  6.0000, 11.8740,  1.8301],\n",
      "         [54.0000, 53.0000,  7.0000,  5.3245,  1.9801],\n",
      "         [55.0000, 53.0000,  7.0000,  5.7878,  1.9935]],\n",
      "\n",
      "        [[50.0000, 49.0000,  0.0000, 20.1425,  1.3759],\n",
      "         [49.0000, 49.0000,  0.0000,  5.5923,  1.4002],\n",
      "         [49.0000, 49.0000,  1.0000, 28.7575,  1.4679],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[50.0000, 49.0000,  0.0000, 24.1774,  1.3899],\n",
      "         [50.0000, 49.0000,  1.0000, 32.3433,  1.4565],\n",
      "         [50.0000, 49.0000,  2.0000, 46.7067,  1.5503],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[50.0000, 49.0000,  0.0000,  7.3573,  1.3627],\n",
      "         [50.0000, 50.0000,  0.0000, 20.0311,  1.3924],\n",
      "         [50.0000, 50.0000,  1.0000, 23.5335,  1.4610],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[50.0000, 49.0000,  0.0000,  5.4130,  1.3543],\n",
      "         [50.0000, 50.0000,  0.0000, 18.1668,  1.3930],\n",
      "         [50.0000, 50.0000,  1.0000, 27.5727,  1.4642],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[50.0000, 49.0000,  0.0000,  8.0048,  1.3480],\n",
      "         [50.0000, 50.0000,  0.0000, 13.9322,  1.3691],\n",
      "         [49.0000, 50.0000,  0.0000, 10.9033,  1.4032],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]]), 'label': tensor([0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,\n",
      "        1, 0, 1, 0, 0, 1, 1, 1]), 'seq_length': tensor([2968, 2498, 2766, 1967, 2781, 2572, 2771, 2432, 2442, 2488, 2644, 2692,\n",
      "        2668, 2664, 2386, 2741, 2584, 1803, 2577, 2783, 2508, 2569, 2494, 2830,\n",
      "        2424, 2733, 2519, 2188, 2700, 2325, 2696, 2619])}\n"
     ]
    }
   ],
   "source": [
    "for i,batch in enumerate(train_loader):\n",
    "    print(i)\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "def test_model(model, test_loader, criterion=None, device=None):\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    # Progress bar for testing\n",
    "    test_loader_tqdm = tqdm(enumerate(test_loader), total=len(test_loader), desc=\"Testing\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i,batch in test_loader_tqdm:\n",
    "            parts = batch[\"part\"].to(device)         # Input point cloud data\n",
    "            labels = batch[\"label\"].to(device)  # Labels\n",
    "            batch_size,seq_len,feat_dim=parts.shape\n",
    "            parts=parts.cpu().numpy().reshape(-1,feat_dim)\n",
    "            qt = QuantileTransformer(output_distribution='normal', random_state=42)\n",
    "            parts = qt.fit_transform(parts)\n",
    "            parts=torch.tensor(parts).reshape(batch_size,seq_len,feat_dim).to(device)\n",
    "\n",
    "            outputs = model(parts)  # Forward pass\n",
    "            loss = criterion(outputs, labels) if criterion else 0  # Compute loss if provided\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)  # Get class prediction\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "            test_loader_tqdm.set_postfix(loss=loss.item())  # Update progress bar\n",
    "\n",
    "    avg_loss = total_loss / len(test_loader) if criterion else 0\n",
    "    accuracy = correct / total_samples * 100\n",
    "\n",
    "    print(f\"Test Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "    return avg_loss , accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# from tqdm import tqdm  # For progress bar\n",
    "# from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "# def train_model(model, train_loader, val_loader, num_epochs=10, learning_rate=5e-4, device=None, save_path=\"./best_model.pth\"):\n",
    "#     if device is None:\n",
    "#         device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#     model.to(device)\n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "#     optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "#     best_val_loss = float(\"inf\")  # Initialize best loss\n",
    "\n",
    "#     for epoch in range(num_epochs):\n",
    "#         model.train()  # Set model to training mode\n",
    "#         running_loss = 0.0\n",
    "\n",
    "#         # Progress bar for training\n",
    "#         train_loader_tqdm = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "#         for i, batch in train_loader_tqdm:\n",
    "#             parts = batch[\"part\"].to(device)         # Input point cloud data\n",
    "#             batch_size,seq_len,feat_dim=parts.shape\n",
    "#             parts=parts.cpu().numpy().reshape(-1,feat_dim)\n",
    "#             qt = QuantileTransformer(output_distribution='normal', random_state=42)\n",
    "#             parts = qt.fit_transform(parts)\n",
    "#             parts=torch.tensor(parts).reshape(batch_size,seq_len,feat_dim).to(device)\n",
    "#             labels = batch[\"label\"].to(device)  # Labels (batch_size,)\n",
    "\n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = model(parts)  # Forward pass\n",
    "#             loss = criterion(outputs, labels)  # Compute loss\n",
    "\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#             running_loss += loss.item()\n",
    "#             train_loader_tqdm.set_postfix(loss=loss.item())  # Update progress bar\n",
    "\n",
    "#         avg_train_loss = running_loss / len(train_loader)\n",
    "\n",
    "#         # Validate the model after each epoch\n",
    "#         val_loss = test_model(model, val_loader, criterion, device)\n",
    "\n",
    "#         print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "#         # Save model if validation loss improves\n",
    "#         if val_loss < best_val_loss:\n",
    "#             best_val_loss = val_loss\n",
    "#             torch.save(model.state_dict(), save_path)\n",
    "#             print(f\"Model saved at epoch {epoch+1} with val loss {val_loss:.4f}\")\n",
    "\n",
    "#     print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=2, learning_rate=5e-4, device=None, save_path=\"./Models/Z_100GeV_50.pth\", log_path=\"./log_summary_Z_100GeV_50.csv\"):\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    best_val_loss = float(\"inf\")  # Initialize best loss\n",
    "\n",
    "    log_data = []  # To store log info for CSV\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        train_loader_tqdm = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "        for i, batch in train_loader_tqdm:\n",
    "            parts = batch[\"part\"].to(device)\n",
    "            batch_size, seq_len, feat_dim = parts.shape\n",
    "            parts = parts.cpu().numpy().reshape(-1, feat_dim)\n",
    "            qt = QuantileTransformer(output_distribution='normal', random_state=42)\n",
    "            parts = qt.fit_transform(parts)\n",
    "            parts = torch.tensor(parts).reshape(batch_size, seq_len, feat_dim).float().to(device)\n",
    "\n",
    "            labels = batch[\"label\"].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(parts)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            train_loader_tqdm.set_postfix(loss=loss.item())\n",
    "\n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "        val_loss, Accuracy = test_model(model, val_loader, criterion, device)\n",
    "         \n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Save log\n",
    "        log_data.append({\n",
    "            \"Epoch\": epoch + 1,\n",
    "            \"Train Loss\": avg_train_loss,\n",
    "            \"Val Loss\": val_loss,\n",
    "            \"Accuracy\": Accuracy,  # Placeholder for accuracy\n",
    "        })\n",
    "\n",
    "        # Save model if validation improves\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(f\"Model saved at epoch {epoch+1} with val loss {val_loss:.4f}\")\n",
    "\n",
    "    # Save the log to CSV\n",
    "    df_log = pd.DataFrame(log_data)\n",
    "    df_log.to_csv(log_path, index=False)\n",
    "    print(f\"Training log saved to {log_path}\")\n",
    "    print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/60: 100%|██████████| 5000/5000 [06:26<00:00, 12.92it/s, loss=0.649]\n",
      "Testing: 100%|██████████| 625/625 [00:44<00:00, 14.13it/s, loss=0.671]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.6765, Accuracy: 57.21%\n",
      "Epoch [1/60], Train Loss: 0.6918, Val Loss: 0.6765\n",
      "Model saved at epoch 1 with val loss 0.6765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/60: 100%|██████████| 5000/5000 [06:22<00:00, 13.09it/s, loss=0.662]\n",
      "Testing: 100%|██████████| 625/625 [00:42<00:00, 14.86it/s, loss=0.634]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.6595, Accuracy: 60.05%\n",
      "Epoch [2/60], Train Loss: 0.6663, Val Loss: 0.6595\n",
      "Model saved at epoch 2 with val loss 0.6595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/60: 100%|██████████| 5000/5000 [06:06<00:00, 13.63it/s, loss=0.694]\n",
      "Testing: 100%|██████████| 625/625 [00:45<00:00, 13.79it/s, loss=0.747]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.6543, Accuracy: 61.17%\n",
      "Epoch [3/60], Train Loss: 0.6619, Val Loss: 0.6543\n",
      "Model saved at epoch 3 with val loss 0.6543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4/60: 100%|██████████| 5000/5000 [06:08<00:00, 13.56it/s, loss=0.729]\n",
      "Testing: 100%|██████████| 625/625 [00:42<00:00, 14.88it/s, loss=0.705]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.6674, Accuracy: 58.23%\n",
      "Epoch [4/60], Train Loss: 0.6578, Val Loss: 0.6674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5/60: 100%|██████████| 5000/5000 [06:09<00:00, 13.53it/s, loss=0.695]\n",
      "Testing: 100%|██████████| 625/625 [00:42<00:00, 14.81it/s, loss=0.659]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.6585, Accuracy: 60.27%\n",
      "Epoch [5/60], Train Loss: 0.6555, Val Loss: 0.6585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6/60: 100%|██████████| 5000/5000 [06:11<00:00, 13.45it/s, loss=0.659]\n",
      "Testing: 100%|██████████| 625/625 [00:42<00:00, 14.79it/s, loss=0.611]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.6510, Accuracy: 61.41%\n",
      "Epoch [6/60], Train Loss: 0.6542, Val Loss: 0.6510\n",
      "Model saved at epoch 6 with val loss 0.6510\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/60: 100%|██████████| 5000/5000 [05:53<00:00, 14.13it/s, loss=0.696]\n",
      "Testing: 100%|██████████| 625/625 [00:38<00:00, 16.41it/s, loss=0.638]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.6478, Accuracy: 62.03%\n",
      "Epoch [7/60], Train Loss: 0.6516, Val Loss: 0.6478\n",
      "Model saved at epoch 7 with val loss 0.6478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8/60: 100%|██████████| 5000/5000 [05:37<00:00, 14.82it/s, loss=0.633]\n",
      "Testing: 100%|██████████| 625/625 [00:37<00:00, 16.58it/s, loss=0.58] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.6480, Accuracy: 61.62%\n",
      "Epoch [8/60], Train Loss: 0.6480, Val Loss: 0.6480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9/60: 100%|██████████| 5000/5000 [05:38<00:00, 14.76it/s, loss=0.622]\n",
      "Testing: 100%|██████████| 625/625 [00:46<00:00, 13.37it/s, loss=0.661]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.6467, Accuracy: 61.70%\n",
      "Epoch [9/60], Train Loss: 0.6467, Val Loss: 0.6467\n",
      "Model saved at epoch 9 with val loss 0.6467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10/60: 100%|██████████| 5000/5000 [05:45<00:00, 14.48it/s, loss=0.624]\n",
      "Testing: 100%|██████████| 625/625 [00:46<00:00, 13.41it/s, loss=0.642]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.6498, Accuracy: 61.53%\n",
      "Epoch [10/60], Train Loss: 0.6453, Val Loss: 0.6498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11/60: 100%|██████████| 5000/5000 [05:44<00:00, 14.50it/s, loss=0.616]\n",
      "Testing: 100%|██████████| 625/625 [00:46<00:00, 13.41it/s, loss=0.609]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.6427, Accuracy: 62.40%\n",
      "Epoch [11/60], Train Loss: 0.6447, Val Loss: 0.6427\n",
      "Model saved at epoch 11 with val loss 0.6427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12/60: 100%|██████████| 5000/5000 [05:45<00:00, 14.49it/s, loss=0.608]\n",
      "Testing: 100%|██████████| 625/625 [00:46<00:00, 13.33it/s, loss=0.724]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.6405, Accuracy: 62.48%\n",
      "Epoch [12/60], Train Loss: 0.6446, Val Loss: 0.6405\n",
      "Model saved at epoch 12 with val loss 0.6405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 13/60: 100%|██████████| 5000/5000 [05:44<00:00, 14.51it/s, loss=0.71] \n",
      "Testing: 100%|██████████| 625/625 [00:46<00:00, 13.38it/s, loss=0.671]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.6419, Accuracy: 62.43%\n",
      "Epoch [13/60], Train Loss: 0.6418, Val Loss: 0.6419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14/60: 100%|██████████| 5000/5000 [05:44<00:00, 14.51it/s, loss=0.596]\n",
      "Testing: 100%|██████████| 625/625 [00:46<00:00, 13.35it/s, loss=0.615]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.6355, Accuracy: 63.01%\n",
      "Epoch [14/60], Train Loss: 0.6422, Val Loss: 0.6355\n",
      "Model saved at epoch 14 with val loss 0.6355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15/60: 100%|██████████| 5000/5000 [05:45<00:00, 14.49it/s, loss=0.615]\n",
      "Testing: 100%|██████████| 625/625 [00:46<00:00, 13.40it/s, loss=0.625]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.6381, Accuracy: 62.92%\n",
      "Epoch [15/60], Train Loss: 0.6410, Val Loss: 0.6381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 16/60: 100%|██████████| 5000/5000 [05:47<00:00, 14.40it/s, loss=0.668]\n",
      "Testing: 100%|██████████| 625/625 [00:46<00:00, 13.37it/s, loss=0.625]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.6340, Accuracy: 63.74%\n",
      "Epoch [16/60], Train Loss: 0.6387, Val Loss: 0.6340\n",
      "Model saved at epoch 16 with val loss 0.6340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 17/60: 100%|██████████| 5000/5000 [05:45<00:00, 14.47it/s, loss=0.66] \n",
      "Testing: 100%|██████████| 625/625 [00:46<00:00, 13.34it/s, loss=0.682]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.6408, Accuracy: 62.84%\n",
      "Epoch [17/60], Train Loss: 0.6389, Val Loss: 0.6408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 18/60: 100%|██████████| 5000/5000 [05:46<00:00, 14.43it/s, loss=0.752]\n",
      "Testing: 100%|██████████| 625/625 [00:47<00:00, 13.15it/s, loss=0.723]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.6368, Accuracy: 63.03%\n",
      "Epoch [18/60], Train Loss: 0.6392, Val Loss: 0.6368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 19/60: 100%|██████████| 5000/5000 [05:46<00:00, 14.43it/s, loss=0.666]\n",
      "Testing: 100%|██████████| 625/625 [00:47<00:00, 13.25it/s, loss=0.621]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.6386, Accuracy: 62.91%\n",
      "Epoch [19/60], Train Loss: 0.6371, Val Loss: 0.6386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 20/60: 100%|██████████| 5000/5000 [05:45<00:00, 14.48it/s, loss=0.692]\n",
      "Testing: 100%|██████████| 625/625 [00:46<00:00, 13.35it/s, loss=0.74] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.6360, Accuracy: 62.90%\n",
      "Epoch [20/60], Train Loss: 0.6371, Val Loss: 0.6360\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 21/60: 100%|██████████| 5000/5000 [05:46<00:00, 14.45it/s, loss=0.56] \n",
      "Testing: 100%|██████████| 625/625 [00:47<00:00, 13.21it/s, loss=0.686]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.6337, Accuracy: 63.70%\n",
      "Epoch [21/60], Train Loss: 0.6376, Val Loss: 0.6337\n",
      "Model saved at epoch 21 with val loss 0.6337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/60: 100%|██████████| 5000/5000 [05:45<00:00, 14.46it/s, loss=0.637]\n",
      "Testing: 100%|██████████| 625/625 [00:46<00:00, 13.36it/s, loss=0.664]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.6312, Accuracy: 63.71%\n",
      "Epoch [22/60], Train Loss: 0.6368, Val Loss: 0.6312\n",
      "Model saved at epoch 22 with val loss 0.6312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 23/60: 100%|██████████| 5000/5000 [05:45<00:00, 14.49it/s, loss=0.639]\n",
      "Testing: 100%|██████████| 625/625 [00:46<00:00, 13.40it/s, loss=0.745]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.6377, Accuracy: 63.04%\n",
      "Epoch [23/60], Train Loss: 0.6359, Val Loss: 0.6377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 24/60: 100%|██████████| 5000/5000 [05:44<00:00, 14.53it/s, loss=0.668]\n",
      "Testing: 100%|██████████| 625/625 [00:47<00:00, 13.22it/s, loss=0.651]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.6362, Accuracy: 63.01%\n",
      "Epoch [24/60], Train Loss: 0.6345, Val Loss: 0.6362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 25/60: 100%|██████████| 5000/5000 [05:46<00:00, 14.43it/s, loss=0.59] \n",
      "Testing: 100%|██████████| 625/625 [00:47<00:00, 13.25it/s, loss=0.659]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.6368, Accuracy: 62.79%\n",
      "Epoch [25/60], Train Loss: 0.6358, Val Loss: 0.6368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 26/60: 100%|██████████| 5000/5000 [05:34<00:00, 14.96it/s, loss=0.538]\n",
      "Testing: 100%|██████████| 625/625 [00:45<00:00, 13.79it/s, loss=0.625]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.6306, Accuracy: 63.38%\n",
      "Epoch [26/60], Train Loss: 0.6338, Val Loss: 0.6306\n",
      "Model saved at epoch 26 with val loss 0.6306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 27/60: 100%|██████████| 5000/5000 [06:14<00:00, 13.34it/s, loss=0.714]\n",
      "Testing: 100%|██████████| 625/625 [00:57<00:00, 10.94it/s, loss=0.64] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.6347, Accuracy: 63.31%\n",
      "Epoch [27/60], Train Loss: 0.6331, Val Loss: 0.6347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 28/60: 100%|██████████| 5000/5000 [07:18<00:00, 11.41it/s, loss=0.626]\n",
      "Testing: 100%|██████████| 625/625 [00:58<00:00, 10.66it/s, loss=0.637]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.6332, Accuracy: 63.14%\n",
      "Epoch [28/60], Train Loss: 0.6334, Val Loss: 0.6332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 29/60: 100%|██████████| 5000/5000 [07:25<00:00, 11.21it/s, loss=0.656]\n",
      "Testing: 100%|██████████| 625/625 [00:58<00:00, 10.77it/s, loss=0.683]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.6292, Accuracy: 63.73%\n",
      "Epoch [29/60], Train Loss: 0.6331, Val Loss: 0.6292\n",
      "Model saved at epoch 29 with val loss 0.6292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 30/60: 100%|██████████| 5000/5000 [07:23<00:00, 11.28it/s, loss=0.656]\n",
      "Testing: 100%|██████████| 625/625 [00:58<00:00, 10.66it/s, loss=0.528]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.6290, Accuracy: 64.05%\n",
      "Epoch [30/60], Train Loss: 0.6328, Val Loss: 0.6290\n",
      "Model saved at epoch 30 with val loss 0.6290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 31/60: 100%|██████████| 5000/5000 [07:24<00:00, 11.26it/s, loss=0.628]\n",
      "Testing: 100%|██████████| 625/625 [00:58<00:00, 10.69it/s, loss=0.67] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.6371, Accuracy: 62.99%\n",
      "Epoch [31/60], Train Loss: 0.6316, Val Loss: 0.6371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 32/60: 100%|██████████| 5000/5000 [07:23<00:00, 11.26it/s, loss=0.678]\n",
      "Testing: 100%|██████████| 625/625 [00:58<00:00, 10.75it/s, loss=0.617]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.6331, Accuracy: 63.10%\n",
      "Epoch [32/60], Train Loss: 0.6317, Val Loss: 0.6331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 33/60: 100%|██████████| 5000/5000 [07:24<00:00, 11.26it/s, loss=0.709]\n",
      "Testing: 100%|██████████| 625/625 [00:57<00:00, 10.85it/s, loss=0.605]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.6344, Accuracy: 62.80%\n",
      "Epoch [33/60], Train Loss: 0.6322, Val Loss: 0.6344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 34/60: 100%|██████████| 5000/5000 [07:25<00:00, 11.22it/s, loss=0.614]\n",
      "Testing: 100%|██████████| 625/625 [00:58<00:00, 10.72it/s, loss=0.647]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.6328, Accuracy: 62.99%\n",
      "Epoch [34/60], Train Loss: 0.6319, Val Loss: 0.6328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 35/60: 100%|██████████| 5000/5000 [07:23<00:00, 11.28it/s, loss=0.697]\n",
      "Testing: 100%|██████████| 625/625 [00:57<00:00, 10.82it/s, loss=0.695]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.6329, Accuracy: 63.21%\n",
      "Epoch [35/60], Train Loss: 0.6329, Val Loss: 0.6329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 36/60: 100%|██████████| 5000/5000 [07:22<00:00, 11.30it/s, loss=0.737]\n",
      "Testing: 100%|██████████| 625/625 [00:58<00:00, 10.67it/s, loss=0.776]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.6324, Accuracy: 63.33%\n",
      "Epoch [36/60], Train Loss: 0.6309, Val Loss: 0.6324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 37/60: 100%|██████████| 5000/5000 [07:17<00:00, 11.42it/s, loss=0.619]\n",
      "Testing: 100%|██████████| 625/625 [00:58<00:00, 10.71it/s, loss=0.705]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.6348, Accuracy: 62.96%\n",
      "Epoch [37/60], Train Loss: 0.6303, Val Loss: 0.6348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 38/60: 100%|██████████| 5000/5000 [07:16<00:00, 11.45it/s, loss=0.67] \n",
      "Testing: 100%|██████████| 625/625 [00:51<00:00, 12.24it/s, loss=0.616]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.6257, Accuracy: 64.49%\n",
      "Epoch [38/60], Train Loss: 0.6291, Val Loss: 0.6257\n",
      "Model saved at epoch 38 with val loss 0.6257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 39/60: 100%|██████████| 5000/5000 [07:00<00:00, 11.88it/s, loss=0.749]\n",
      "Testing: 100%|██████████| 625/625 [00:53<00:00, 11.73it/s, loss=0.647]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.6355, Accuracy: 63.75%\n",
      "Epoch [39/60], Train Loss: 0.6301, Val Loss: 0.6355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 40/60: 100%|██████████| 5000/5000 [07:01<00:00, 11.86it/s, loss=0.601]\n",
      "Testing: 100%|██████████| 625/625 [00:50<00:00, 12.31it/s, loss=0.763]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.6316, Accuracy: 63.83%\n",
      "Epoch [40/60], Train Loss: 0.6283, Val Loss: 0.6316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 41/60: 100%|██████████| 5000/5000 [07:03<00:00, 11.81it/s, loss=0.601]\n",
      "Testing: 100%|██████████| 625/625 [00:55<00:00, 11.18it/s, loss=0.607]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.6239, Accuracy: 64.11%\n",
      "Epoch [41/60], Train Loss: 0.6294, Val Loss: 0.6239\n",
      "Model saved at epoch 41 with val loss 0.6239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 42/60: 100%|██████████| 5000/5000 [06:58<00:00, 11.96it/s, loss=0.653]\n",
      "Testing: 100%|██████████| 625/625 [00:55<00:00, 11.17it/s, loss=0.643]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.6315, Accuracy: 63.70%\n",
      "Epoch [42/60], Train Loss: 0.6290, Val Loss: 0.6315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 43/60: 100%|██████████| 5000/5000 [07:02<00:00, 11.84it/s, loss=0.639]\n",
      "Testing: 100%|██████████| 625/625 [00:55<00:00, 11.36it/s, loss=0.588]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.6262, Accuracy: 64.56%\n",
      "Epoch [43/60], Train Loss: 0.6285, Val Loss: 0.6262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 44/60: 100%|██████████| 5000/5000 [06:55<00:00, 12.05it/s, loss=0.551]\n",
      "Testing: 100%|██████████| 625/625 [00:53<00:00, 11.67it/s, loss=0.608]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.6253, Accuracy: 64.18%\n",
      "Epoch [44/60], Train Loss: 0.6272, Val Loss: 0.6253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 45/60: 100%|██████████| 5000/5000 [06:52<00:00, 12.13it/s, loss=0.643]\n",
      "Testing: 100%|██████████| 625/625 [00:55<00:00, 11.16it/s, loss=0.662]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.6266, Accuracy: 64.14%\n",
      "Epoch [45/60], Train Loss: 0.6268, Val Loss: 0.6266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 46/60: 100%|██████████| 5000/5000 [06:51<00:00, 12.16it/s, loss=0.628]\n",
      "Testing: 100%|██████████| 625/625 [00:54<00:00, 11.42it/s, loss=0.681]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.6238, Accuracy: 64.52%\n",
      "Epoch [46/60], Train Loss: 0.6286, Val Loss: 0.6238\n",
      "Model saved at epoch 46 with val loss 0.6238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 47/60: 100%|██████████| 5000/5000 [06:50<00:00, 12.19it/s, loss=0.655]\n",
      "Testing: 100%|██████████| 625/625 [00:52<00:00, 11.83it/s, loss=0.686]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.6263, Accuracy: 64.39%\n",
      "Epoch [47/60], Train Loss: 0.6284, Val Loss: 0.6263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 48/60: 100%|██████████| 5000/5000 [06:41<00:00, 12.45it/s, loss=0.577]\n",
      "Testing: 100%|██████████| 625/625 [00:52<00:00, 11.88it/s, loss=0.66] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.6271, Accuracy: 64.15%\n",
      "Epoch [48/60], Train Loss: 0.6271, Val Loss: 0.6271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 49/60: 100%|██████████| 5000/5000 [06:38<00:00, 12.55it/s, loss=0.672]\n",
      "Testing: 100%|██████████| 625/625 [00:51<00:00, 12.07it/s, loss=0.657]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.6263, Accuracy: 64.27%\n",
      "Epoch [49/60], Train Loss: 0.6260, Val Loss: 0.6263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 50/60: 100%|██████████| 5000/5000 [06:39<00:00, 12.52it/s, loss=0.577]\n",
      "Testing: 100%|██████████| 625/625 [00:52<00:00, 11.97it/s, loss=0.699]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.6217, Accuracy: 64.61%\n",
      "Epoch [50/60], Train Loss: 0.6266, Val Loss: 0.6217\n",
      "Model saved at epoch 50 with val loss 0.6217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 51/60: 100%|██████████| 5000/5000 [06:39<00:00, 12.51it/s, loss=0.577]\n",
      "Testing: 100%|██████████| 625/625 [00:52<00:00, 11.90it/s, loss=0.701]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.6264, Accuracy: 64.14%\n",
      "Epoch [51/60], Train Loss: 0.6271, Val Loss: 0.6264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 52/60: 100%|██████████| 5000/5000 [06:12<00:00, 13.41it/s, loss=0.679]\n",
      "Testing: 100%|██████████| 625/625 [00:47<00:00, 13.28it/s, loss=0.714]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.6246, Accuracy: 64.73%\n",
      "Epoch [52/60], Train Loss: 0.6238, Val Loss: 0.6246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 53/60: 100%|██████████| 5000/5000 [05:58<00:00, 13.93it/s, loss=0.601]\n",
      "Testing: 100%|██████████| 625/625 [00:46<00:00, 13.37it/s, loss=0.523]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.6224, Accuracy: 64.73%\n",
      "Epoch [53/60], Train Loss: 0.6252, Val Loss: 0.6224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 54/60: 100%|██████████| 5000/5000 [05:59<00:00, 13.92it/s, loss=0.562]\n",
      "Testing: 100%|██████████| 625/625 [00:46<00:00, 13.44it/s, loss=0.587]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.6277, Accuracy: 64.13%\n",
      "Epoch [54/60], Train Loss: 0.6250, Val Loss: 0.6277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 55/60: 100%|██████████| 5000/5000 [05:58<00:00, 13.94it/s, loss=0.552]\n",
      "Testing: 100%|██████████| 625/625 [00:46<00:00, 13.37it/s, loss=0.702]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.6305, Accuracy: 64.01%\n",
      "Epoch [55/60], Train Loss: 0.6253, Val Loss: 0.6305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 56/60: 100%|██████████| 5000/5000 [05:59<00:00, 13.90it/s, loss=0.605]\n",
      "Testing: 100%|██████████| 625/625 [00:46<00:00, 13.33it/s, loss=0.608]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.6281, Accuracy: 63.93%\n",
      "Epoch [56/60], Train Loss: 0.6247, Val Loss: 0.6281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 57/60: 100%|██████████| 5000/5000 [05:59<00:00, 13.91it/s, loss=0.603]\n",
      "Testing: 100%|██████████| 625/625 [00:47<00:00, 13.28it/s, loss=0.671]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.6264, Accuracy: 64.42%\n",
      "Epoch [57/60], Train Loss: 0.6264, Val Loss: 0.6264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 58/60: 100%|██████████| 5000/5000 [05:59<00:00, 13.92it/s, loss=0.669]\n",
      "Testing: 100%|██████████| 625/625 [00:46<00:00, 13.31it/s, loss=0.581]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.6243, Accuracy: 64.27%\n",
      "Epoch [58/60], Train Loss: 0.6245, Val Loss: 0.6243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 59/60: 100%|██████████| 5000/5000 [05:59<00:00, 13.91it/s, loss=0.561]\n",
      "Testing: 100%|██████████| 625/625 [00:46<00:00, 13.32it/s, loss=0.567]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.6252, Accuracy: 64.45%\n",
      "Epoch [59/60], Train Loss: 0.6253, Val Loss: 0.6252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 60/60: 100%|██████████| 5000/5000 [05:59<00:00, 13.91it/s, loss=0.547]\n",
      "Testing: 100%|██████████| 625/625 [00:46<00:00, 13.36it/s, loss=0.592]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.6230, Accuracy: 64.84%\n",
      "Epoch [60/60], Train Loss: 0.6241, Val Loss: 0.6230\n",
      "Training log saved to ./log_summary_Z_100GeV_50.csv\n",
      "Training complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_model(model, train_loader,val_loader=val_loader, num_epochs=60, learning_rate=5e-4, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeepSet(\n",
       "  (sequential): ModuleList(\n",
       "    (0): DeepSetLayer(\n",
       "      (Gamma): Linear(in_features=5, out_features=80, bias=True)\n",
       "      (Lambda): Linear(in_features=5, out_features=80, bias=True)\n",
       "      (bn): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "    )\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): DeepSetLayer(\n",
       "      (Gamma): Linear(in_features=80, out_features=120, bias=True)\n",
       "      (Lambda): Linear(in_features=80, out_features=120, bias=True)\n",
       "      (bn): BatchNorm1d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "    )\n",
       "    (3): LeakyReLU(negative_slope=0.01)\n",
       "    (4): DeepSetLayer(\n",
       "      (Gamma): Linear(in_features=120, out_features=70, bias=True)\n",
       "      (Lambda): Linear(in_features=120, out_features=70, bias=True)\n",
       "      (bn): BatchNorm1d(70, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "    )\n",
       "    (5): LeakyReLU(negative_slope=0.01)\n",
       "    (6): DeepSetLayer(\n",
       "      (Gamma): Linear(in_features=70, out_features=50, bias=True)\n",
       "      (Lambda): Linear(in_features=70, out_features=50, bias=True)\n",
       "      (bn): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "    )\n",
       "    (7): LeakyReLU(negative_slope=0.01)\n",
       "    (8): DeepSetLayer(\n",
       "      (Gamma): Linear(in_features=50, out_features=8, bias=True)\n",
       "      (Lambda): Linear(in_features=50, out_features=8, bias=True)\n",
       "      (bn): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "    )\n",
       "    (9): DeepSetLayer(\n",
       "      (Gamma): Linear(in_features=8, out_features=2, bias=True)\n",
       "      (Lambda): Linear(in_features=8, out_features=2, bias=True)\n",
       "      (bn): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_test = MODEL\n",
    "model_test.load_state_dict(torch.load(\"./Models/Z_100GeV_50.pth\",weights_only=True))\n",
    "model_test.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45292\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in MODEL.parameters())\n",
    "print(total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader, criterion, device, return_accuracy=False):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    data_loader_tqdm = tqdm(enumerate(data_loader), desc=\"Testing\",total=len(data_loader))\n",
    "    with torch.no_grad():\n",
    "        for i,batch in data_loader_tqdm:\n",
    "            parts = batch[\"part\"].to(device)\n",
    "            batch_size,seq_len,feat_dim=parts.shape\n",
    "            parts=parts.cpu().numpy().reshape(-1,feat_dim)\n",
    "            qt = QuantileTransformer(output_distribution='normal', random_state=42)\n",
    "            parts = qt.fit_transform(parts)\n",
    "            parts=torch.tensor(parts).reshape(batch_size,seq_len,feat_dim).to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "            \n",
    "            outputs = model(parts)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            if return_accuracy:\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "    \n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    accuracy = 100 * correct / total if return_accuracy else None\n",
    "    \n",
    "    return (avg_loss, f\"{accuracy}%\") if return_accuracy else avg_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 625/625 [01:10<00:00,  8.82it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.6210454919338226, '65.155%')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(model_test, test_loader, nn.CrossEntropyLoss(), device, return_accuracy=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
